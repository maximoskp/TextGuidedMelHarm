{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import SeparatedMelHarmTextDataset, MelHarmTextCollatorForSeq2Seq\n",
    "import os\n",
    "import numpy as np\n",
    "from harmony_tokenizers_m21 import ChordSymbolTokenizer, RootTypeTokenizer, \\\n",
    "    PitchClassTokenizer, RootPCTokenizer, GCTRootPCTokenizer, \\\n",
    "    GCTSymbolTokenizer, GCTRootTypeTokenizer, MelodyPitchTokenizer, \\\n",
    "    MergedMelHarmTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BartForConditionalGeneration, BartConfig, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from models import TextGuidedHarmonizationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "chordSymbolTokenizer = ChordSymbolTokenizer.from_pretrained('saved_tokenizers/ChordSymbolTokenizer')\n",
    "rootTypeTokenizer = RootTypeTokenizer.from_pretrained('saved_tokenizers/RootTypeTokenizer')\n",
    "pitchClassTokenizer = PitchClassTokenizer.from_pretrained('saved_tokenizers/PitchClassTokenizer')\n",
    "rootPCTokenizer = RootPCTokenizer.from_pretrained('saved_tokenizers/RootPCTokenizer')\n",
    "melodyPitchTokenizer = MelodyPitchTokenizer.from_pretrained('saved_tokenizers/MelodyPitchTokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_chordSymbolTokenizer = MergedMelHarmTokenizer(melodyPitchTokenizer, chordSymbolTokenizer)\n",
    "m_rootTypeTokenizer = MergedMelHarmTokenizer(melodyPitchTokenizer, rootTypeTokenizer)\n",
    "m_pitchClassTokenizer = MergedMelHarmTokenizer(melodyPitchTokenizer, pitchClassTokenizer)\n",
    "m_rootPCTokenizer = MergedMelHarmTokenizer(melodyPitchTokenizer, rootPCTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = m_chordSymbolTokenizer\n",
    "# tokenizer_name = 'ChordSymbolTokenizer'\n",
    "# tokenizer = m_rootTypeTokenizer\n",
    "# tokenizer_name = 'RootTypeTokenizer'\n",
    "# tokenizer = m_pitchClassTokenizer\n",
    "# tokenizer_name = 'PitchClassTokenizer'\n",
    "tokenizer = m_rootPCTokenizer\n",
    "tokenizer_name = 'RootPCTokenizer'\n",
    "\n",
    "root_dir = '/media/maindisk/maximos/data/hooktheory_test'\n",
    "dataset = SeparatedMelHarmTextDataset(root_dir, tokenizer, max_length=512, num_bars=64)\n",
    "def create_data_collator(tokenizer, model):\n",
    "    return MelHarmTextCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True)\n",
    "# end create_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_name = 'cpu'\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartDecoder(\n",
       "  (embed_tokens): BartScaledWordEmbedding(221, 512, padding_idx=1)\n",
       "  (embed_positions): BartLearnedPositionalEmbedding(514, 512)\n",
       "  (layers): ModuleList(\n",
       "    (0-7): 8 x BartDecoderLayer(\n",
       "      (self_attn): BartSdpaAttention(\n",
       "        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (activation_fn): GELUActivation()\n",
       "      (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder_attn): BartSdpaAttention(\n",
       "        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_config = BartConfig(\n",
    "    vocab_size=len(tokenizer.vocab),\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    decoder_start_token_id=tokenizer.bos_token_id,\n",
    "    forced_eos_token_id=tokenizer.eos_token_id,\n",
    "    max_position_embeddings=512,\n",
    "    encoder_layers=8,\n",
    "    encoder_attention_heads=8,\n",
    "    encoder_ffn_dim=512,\n",
    "    decoder_layers=8,\n",
    "    decoder_attention_heads=8,\n",
    "    decoder_ffn_dim=512,\n",
    "    d_model=512,\n",
    "    encoder_layerdrop=0.3,\n",
    "    decoder_layerdrop=0.3,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "bart = BartForConditionalGeneration(bart_config)\n",
    "\n",
    "bart_path = 'saved_models/bart/' + tokenizer_name + '/' + tokenizer_name + '.pt'\n",
    "checkpoint = torch.load(bart_path, map_location=device_name, weights_only=True)\n",
    "bart.load_state_dict(checkpoint)\n",
    "\n",
    "bart.to(device)\n",
    "bart.eval()\n",
    "\n",
    "bart_encoder, bart_decoder = bart.get_encoder(), bart.get_decoder()\n",
    "bart_encoder.to(device)\n",
    "bart_decoder.to(device)\n",
    "\n",
    "# # Freeze BART parameters\n",
    "# for param in bart_encoder.parameters():\n",
    "#     param.requires_grad = False\n",
    "# for param in bart_encoder.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = create_data_collator(tokenizer, model=bart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maximos/miniconda3/envs/torch/lib/python3.12/site-packages/music21/stream/base.py:3694: Music21DeprecationWarning: .flat is deprecated.  Call .flatten() instead\n",
      "  return self.iter().getElementsByClass(classFilterList)\n"
     ]
    }
   ],
   "source": [
    "b = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'harmony_input_ids', 'labels', 'decoder_input_ids', 'txt'])\n"
     ]
    }
   ],
   "source": [
    "print(b.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 196,    6,   95,  202,  218,  209,  119,  204,  220,  211,  123,  206,\n",
      "          209,  213,    6,   95,  206,  209,  213,    6,   95,  204,  220,  211,\n",
      "          119,  206,  209,  213,  123,  201,  216,  220,    6,   95,  201,  216,\n",
      "          220,  115,  199,  214,  218,  119,  201,  216,  220,    6,   95,  202,\n",
      "          218,  209,  119,  204,  220,  211,  123,  206,  209,  213,    6,   95,\n",
      "          206,  209,  213,    6,   95,  204,  220,  211,  119,  197,  213,  216,\n",
      "          123,  197,  213,  216,    6,   95,  197,  213,  216,    3, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100]])\n"
     ]
    }
   ],
   "source": [
    "print(b['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/maximos/miniconda3/envs/torch/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = TextGuidedHarmonizationModel(bart, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input_ids = b['input_ids'].to(device)\n",
    "melody_attention_mask = b['attention_mask'].to(device)\n",
    "harmony_input_ids = b['harmony_input_ids'].to(device)\n",
    "labels = b['labels'].to(device)\n",
    "texts = b['txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hat_h:  torch.Size([1, 120, 512])\n",
      "labels:  torch.Size([1, 392])\n"
     ]
    }
   ],
   "source": [
    "decoder_loss, decoder_logits = model(model_input_ids, melody_attention_mask, harmony_input_ids, texts, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5530, grad_fn=<NllLossBackward0>)\n",
      "tensor([[[ -8.9886,  -9.1118,   2.4815,  ...,   0.3629,   0.7342,   0.6897],\n",
      "         [-22.2846, -22.1352,  -2.1934,  ...,   0.3874,  -0.8075,   0.7557],\n",
      "         [-20.6434, -20.6101,  -1.5172,  ...,  -2.3137,  -3.0728,  -2.0902],\n",
      "         ...,\n",
      "         [-12.8782, -12.6370,  -2.0237,  ...,  -1.3478,  -0.8248,  -1.4532],\n",
      "         [-12.9575, -12.7007,  -2.0124,  ...,  -1.8641,  -1.0179,  -1.7308],\n",
      "         [-13.0036, -12.7806,  -2.0334,  ...,  -1.9939,  -1.4933,  -2.4119]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_loss)\n",
    "print(decoder_logits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
