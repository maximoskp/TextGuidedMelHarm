{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maximos/miniconda3/envs/torch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data_utils import SeparatedMelHarmTextDataset, MelHarmTextCollatorForSeq2Seq\n",
    "import os\n",
    "import numpy as np\n",
    "from harmony_tokenizers_m21 import ChordSymbolTokenizer, RootTypeTokenizer, \\\n",
    "    PitchClassTokenizer, RootPCTokenizer, GCTRootPCTokenizer, \\\n",
    "    GCTSymbolTokenizer, GCTRootTypeTokenizer, MelodyPitchTokenizer, \\\n",
    "    MergedMelHarmTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BartForConditionalGeneration, BartConfig, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from models import TextGuidedHarmonizationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chordSymbolTokenizer = ChordSymbolTokenizer.from_pretrained('saved_tokenizers/ChordSymbolTokenizer')\n",
    "rootTypeTokenizer = RootTypeTokenizer.from_pretrained('saved_tokenizers/RootTypeTokenizer')\n",
    "pitchClassTokenizer = PitchClassTokenizer.from_pretrained('saved_tokenizers/PitchClassTokenizer')\n",
    "rootPCTokenizer = RootPCTokenizer.from_pretrained('saved_tokenizers/RootPCTokenizer')\n",
    "melodyPitchTokenizer = MelodyPitchTokenizer.from_pretrained('saved_tokenizers/MelodyPitchTokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_chordSymbolTokenizer = MergedMelHarmTokenizer(melodyPitchTokenizer, chordSymbolTokenizer)\n",
    "m_rootTypeTokenizer = MergedMelHarmTokenizer(melodyPitchTokenizer, rootTypeTokenizer)\n",
    "m_pitchClassTokenizer = MergedMelHarmTokenizer(melodyPitchTokenizer, pitchClassTokenizer)\n",
    "m_rootPCTokenizer = MergedMelHarmTokenizer(melodyPitchTokenizer, rootPCTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = m_chordSymbolTokenizer\n",
    "# tokenizer_name = 'ChordSymbolTokenizer'\n",
    "# tokenizer = m_rootTypeTokenizer\n",
    "# tokenizer_name = 'RootTypeTokenizer'\n",
    "# tokenizer = m_pitchClassTokenizer\n",
    "# tokenizer_name = 'PitchClassTokenizer'\n",
    "tokenizer = m_rootPCTokenizer\n",
    "tokenizer_name = 'RootPCTokenizer'\n",
    "\n",
    "train_dir = '/media/maindisk/maximos/data/hooktheory_train'\n",
    "test_dir = '/media/maindisk/maximos/data/hooktheory_test'\n",
    "\n",
    "train_dataset = SeparatedMelHarmTextDataset(\n",
    "    train_dir,\n",
    "    tokenizer,\n",
    "    max_length=512,\n",
    "    num_bars=64,\n",
    "    description_mode='specific_chord',\n",
    "    alteration=True\n",
    ")\n",
    "\n",
    "test_dataset = SeparatedMelHarmTextDataset(\n",
    "    test_dir,\n",
    "    tokenizer,\n",
    "    max_length=512,\n",
    "    num_bars=64,\n",
    "    description_mode='specific_chord',\n",
    "    alteration=True\n",
    ")\n",
    "\n",
    "def create_data_collator(tokenizer, model):\n",
    "    return MelHarmTextCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True)\n",
    "# end create_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([196, 196,   6,  95, 197, 213, 216, 111, 197, 211, 216,   6,  95, 202,\n",
      "        218, 209, 107, 197, 211, 216,   6,  95, 197, 213, 216, 111, 197, 211,\n",
      "        216,   6,  95, 197, 214, 216, 107, 202, 218, 209,   6,  95, 197, 213,\n",
      "        216, 111, 197, 211, 216,   6,  95, 202, 218, 209, 107, 197, 211, 216,\n",
      "          6,  95, 207, 211, 214, 109, 202, 218, 209,   6,  95, 197, 213, 216,\n",
      "        107, 204, 220, 211,   6,  95, 197, 213, 216, 111, 197, 211, 216,   6,\n",
      "         95, 197, 213, 216, 218, 216,   6,  95, 197, 213, 216, 111, 197, 211,\n",
      "        216,   6,  95, 197, 214, 216, 107, 202, 218, 209,   6,  95, 197, 213,\n",
      "        216, 111, 197, 211, 216,   6,  95, 202, 218, 209, 107, 197, 211, 216,\n",
      "          6,  95, 207, 211, 214, 109, 202, 218, 209,   6,  95, 197, 213, 216,\n",
      "        107, 204, 220, 211,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maximos/miniconda3/envs/torch/lib/python3.12/site-packages/music21/stream/base.py:3694: Music21DeprecationWarning: .flat is deprecated.  Call .flatten() instead\n",
      "  return self.iter().getElementsByClass(classFilterList)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0]['harmony_input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device_name = 'cpu'\n",
    "device_name = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartDecoder(\n",
       "  (embed_tokens): BartScaledWordEmbedding(221, 512, padding_idx=1)\n",
       "  (embed_positions): BartLearnedPositionalEmbedding(514, 512)\n",
       "  (layers): ModuleList(\n",
       "    (0-7): 8 x BartDecoderLayer(\n",
       "      (self_attn): BartSdpaAttention(\n",
       "        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (activation_fn): GELUActivation()\n",
       "      (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder_attn): BartSdpaAttention(\n",
       "        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_config = BartConfig(\n",
    "    vocab_size=len(tokenizer.vocab),\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    decoder_start_token_id=tokenizer.bos_token_id,\n",
    "    forced_eos_token_id=tokenizer.eos_token_id,\n",
    "    max_position_embeddings=512,\n",
    "    encoder_layers=8,\n",
    "    encoder_attention_heads=8,\n",
    "    encoder_ffn_dim=512,\n",
    "    decoder_layers=8,\n",
    "    decoder_attention_heads=8,\n",
    "    decoder_ffn_dim=512,\n",
    "    d_model=512,\n",
    "    encoder_layerdrop=0.3,\n",
    "    decoder_layerdrop=0.3,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "bart = BartForConditionalGeneration(bart_config)\n",
    "\n",
    "bart_path = 'saved_models/bart/' + tokenizer_name + '/' + tokenizer_name + '.pt'\n",
    "checkpoint = torch.load(bart_path, map_location=device_name, weights_only=True)\n",
    "bart.load_state_dict(checkpoint)\n",
    "\n",
    "bart.to(device)\n",
    "bart.eval()\n",
    "\n",
    "bart_encoder, bart_decoder = bart.get_encoder(), bart.get_decoder()\n",
    "bart_encoder.to(device)\n",
    "bart_decoder.to(device)\n",
    "\n",
    "# # Freeze BART parameters\n",
    "# for param in bart_encoder.parameters():\n",
    "#     param.requires_grad = False\n",
    "# for param in bart_encoder.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = create_data_collator(tokenizer, model=bart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collator)\n",
    "valloader = DataLoader(test_dataset, batch_size=16, shuffle=True, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maximos/miniconda3/envs/torch/lib/python3.12/site-packages/music21/stream/base.py:3694: Music21DeprecationWarning: .flat is deprecated.  Call .flatten() instead\n",
      "  return self.iter().getElementsByClass(classFilterList)\n",
      "/home/maximos/miniconda3/envs/torch/lib/python3.12/site-packages/transformers/data/data_collator.py:656: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n",
      "/media/maindisk/maximos/repos/TextGuidedMelHarm/data_utils.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(h) for h in harmony_inputs],\n"
     ]
    }
   ],
   "source": [
    "b = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels', 'decoder_input_ids', 'txt', 'harmony_input_ids'])\n"
     ]
    }
   ],
   "source": [
    "print(b.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 196,    6,   95,  ..., -100, -100, -100],\n",
      "        [ 196,    6,    6,  ..., -100, -100, -100],\n",
      "        [ 196,    6,   95,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [ 196,    6,   95,  ..., -100, -100, -100],\n",
      "        [ 196,    6,   95,  ..., -100, -100, -100],\n",
      "        [ 196,    6,    6,  ..., -100, -100, -100]])\n"
     ]
    }
   ],
   "source": [
    "print(b['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/maximos/miniconda3/envs/torch/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = TextGuidedHarmonizationModel(bart, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input_ids = b['input_ids'].to(device)\n",
    "melody_attention_mask = b['attention_mask'].to(device)\n",
    "harmony_input_ids = b['harmony_input_ids'].to(device)\n",
    "labels = b['labels'].to(device)\n",
    "texts = b['txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(model_input_ids, melody_attention_mask, harmony_input_ids, texts, labels=labels)\n",
    "decoder_loss = output['loss']\n",
    "decoder_logits = output['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1272, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[[-8.5915e+00, -8.7255e+00,  2.5597e+00,  ...,  4.8873e-01,\n",
      "           8.6521e-01,  6.4023e-01],\n",
      "         [-2.2204e+01, -2.2071e+01, -2.0999e+00,  ...,  4.6540e-01,\n",
      "          -7.4663e-01,  5.8135e-01],\n",
      "         [-2.0728e+01, -2.0710e+01, -1.4445e+00,  ..., -2.1973e+00,\n",
      "          -3.0354e+00, -2.2271e+00],\n",
      "         ...,\n",
      "         [-6.4683e+00, -6.2666e+00, -6.7605e-01,  ..., -1.2500e+00,\n",
      "          -2.0705e+00, -1.1193e+00],\n",
      "         [-6.3887e+00, -6.2053e+00, -6.5636e-01,  ..., -1.2764e+00,\n",
      "          -2.1578e+00, -1.2972e+00],\n",
      "         [-6.4046e+00, -6.2078e+00, -6.7884e-01,  ..., -1.3459e+00,\n",
      "          -1.9831e+00, -1.1115e+00]],\n",
      "\n",
      "        [[-8.8575e+00, -8.9734e+00,  2.5176e+00,  ...,  4.1744e-01,\n",
      "           7.4860e-01,  6.6337e-01],\n",
      "         [-2.2240e+01, -2.2079e+01, -2.1657e+00,  ...,  4.8006e-01,\n",
      "          -8.0228e-01,  7.0033e-01],\n",
      "         [-2.0656e+01, -2.0613e+01, -1.4958e+00,  ..., -2.2319e+00,\n",
      "          -3.0696e+00, -2.1307e+00],\n",
      "         ...,\n",
      "         [-5.5190e+00, -5.3694e+00, -1.0314e+00,  ..., -1.0599e+00,\n",
      "          -1.7644e+00, -1.5581e+00],\n",
      "         [-5.4539e+00, -5.3211e+00, -1.0154e+00,  ..., -1.2499e+00,\n",
      "          -1.9813e+00, -1.8422e+00],\n",
      "         [-5.4325e+00, -5.2836e+00, -1.0372e+00,  ..., -1.2518e+00,\n",
      "          -1.6750e+00, -1.5248e+00]],\n",
      "\n",
      "        [[-8.8155e+00, -8.9276e+00,  2.5274e+00,  ...,  3.9974e-01,\n",
      "           7.5838e-01,  7.0991e-01],\n",
      "         [-2.2277e+01, -2.2113e+01, -2.1642e+00,  ...,  4.5784e-01,\n",
      "          -8.2491e-01,  7.2790e-01],\n",
      "         [-2.0646e+01, -2.0601e+01, -1.4932e+00,  ..., -2.2377e+00,\n",
      "          -3.0724e+00, -2.0842e+00],\n",
      "         ...,\n",
      "         [-6.2189e+00, -6.0747e+00, -8.4833e-01,  ..., -1.1106e+00,\n",
      "          -1.9215e+00, -7.6955e-01],\n",
      "         [-6.1569e+00, -6.0289e+00, -8.2672e-01,  ..., -1.1790e+00,\n",
      "          -2.0264e+00, -9.5476e-01],\n",
      "         [-6.1676e+00, -6.0266e+00, -8.5615e-01,  ..., -1.2573e+00,\n",
      "          -1.7936e+00, -7.3012e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-8.8824e+00, -9.0153e+00,  2.5037e+00,  ...,  3.9050e-01,\n",
      "           7.2120e-01,  6.6054e-01],\n",
      "         [-2.2258e+01, -2.2122e+01, -2.1624e+00,  ...,  4.1935e-01,\n",
      "          -8.8589e-01,  6.5664e-01],\n",
      "         [-2.0698e+01, -2.0675e+01, -1.5032e+00,  ..., -2.2409e+00,\n",
      "          -3.1202e+00, -2.1631e+00],\n",
      "         ...,\n",
      "         [-5.9898e+00, -5.7899e+00, -6.4186e-01,  ..., -1.5405e+00,\n",
      "          -1.3892e+00, -9.4268e-01],\n",
      "         [-5.8906e+00, -5.7119e+00, -6.3427e-01,  ..., -1.5713e+00,\n",
      "          -1.4883e+00, -1.0931e+00],\n",
      "         [-5.8998e+00, -5.7080e+00, -6.4948e-01,  ..., -1.6164e+00,\n",
      "          -1.2949e+00, -8.8944e-01]],\n",
      "\n",
      "        [[-8.8746e+00, -8.9897e+00,  2.5339e+00,  ...,  4.5797e-01,\n",
      "           8.1117e-01,  6.5359e-01],\n",
      "         [-2.2233e+01, -2.2071e+01, -2.1398e+00,  ...,  5.2730e-01,\n",
      "          -7.7566e-01,  6.7078e-01],\n",
      "         [-2.0659e+01, -2.0614e+01, -1.4772e+00,  ..., -2.2147e+00,\n",
      "          -3.0501e+00, -2.1453e+00],\n",
      "         ...,\n",
      "         [-1.1043e+01, -1.0895e+01, -1.5817e+00,  ..., -8.6140e-01,\n",
      "          -2.8558e-02, -1.0515e+00],\n",
      "         [-1.0974e+01, -1.0834e+01, -1.5365e+00,  ..., -1.0951e+00,\n",
      "          -3.1832e-01, -1.4115e+00],\n",
      "         [-1.1003e+01, -1.0847e+01, -1.5764e+00,  ..., -9.6200e-01,\n",
      "          -3.6216e-03, -1.0950e+00]],\n",
      "\n",
      "        [[-8.8374e+00, -8.9545e+00,  2.5143e+00,  ...,  3.8162e-01,\n",
      "           7.2098e-01,  6.9524e-01],\n",
      "         [-2.2233e+01, -2.2075e+01, -2.1709e+00,  ...,  4.2672e-01,\n",
      "          -8.6740e-01,  7.0899e-01],\n",
      "         [-2.0586e+01, -2.0547e+01, -1.4959e+00,  ..., -2.2607e+00,\n",
      "          -3.0986e+00, -2.0997e+00],\n",
      "         ...,\n",
      "         [-6.5919e+00, -6.3264e+00, -2.2361e-01,  ..., -1.0214e+00,\n",
      "          -1.5291e+00, -4.4442e-01],\n",
      "         [-6.5227e+00, -6.2746e+00, -2.2633e-01,  ..., -1.0193e+00,\n",
      "          -1.5441e+00, -5.3339e-01],\n",
      "         [-6.5117e+00, -6.2531e+00, -2.3656e-01,  ..., -1.1205e+00,\n",
      "          -1.3857e+00, -3.2818e-01]]], device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_loss)\n",
    "print(decoder_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "# Define optimizer (only update trainable parameters)\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n",
    "\n",
    "# Learning rate scheduler\n",
    "num_training_steps = len(trainloader) * epochs\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextGuidedHarmonizationModel(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (bart): BartForConditionalGeneration(\n",
       "    (model): BartModel(\n",
       "      (shared): BartScaledWordEmbedding(221, 512, padding_idx=1)\n",
       "      (encoder): BartEncoder(\n",
       "        (embed_tokens): BartScaledWordEmbedding(221, 512, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(514, 512)\n",
       "        (layers): ModuleList(\n",
       "          (0-7): 8 x BartEncoderLayer(\n",
       "            (self_attn): BartSdpaAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): BartDecoder(\n",
       "        (embed_tokens): BartScaledWordEmbedding(221, 512, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(514, 512)\n",
       "        (layers): ModuleList(\n",
       "          (0-7): 8 x BartDecoderLayer(\n",
       "            (self_attn): BartSdpaAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartSdpaAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=512, out_features=221, bias=False)\n",
       "  )\n",
       "  (attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (lstm): LSTM(512, 512, batch_first=True)\n",
       "  (condition_proj): Linear(in_features=768, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 | trn:   0%|          | 0/855 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maximos/miniconda3/envs/torch/lib/python3.12/site-packages/music21/stream/base.py:3694: Music21DeprecationWarning: .flat is deprecated.  Call .flatten() instead\n",
      "  return self.iter().getElementsByClass(classFilterList)\n",
      "/media/maindisk/maximos/repos/TextGuidedMelHarm/data_utils.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(h) for h in harmony_inputs],\n",
      "Epoch 0 | trn:  15%|█▍        | 128/855 [01:10<06:38,  1.82batch/s, accuracy=0.8, loss=0.667]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(trainloader, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tepoch:\n\u001b[1;32m     14\u001b[0m     tepoch\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | trn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtepoch\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_input_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmelody_attention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/media/maindisk/maximos/repos/TextGuidedMelHarm/data_utils.py:70\u001b[0m, in \u001b[0;36mSeparatedMelHarmTextDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# make description of specific (e.g., the third) bar\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malteration:\n\u001b[0;32m---> 70\u001b[0m     txt, label_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerged_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchange_and_describe_tokens_list_at_random_bar\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoded\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_tokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_harmony_position\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescription_mode\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerged_tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(label_tokens))\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/media/maindisk/maximos/repos/TextGuidedMelHarm/harmony_tokenizers_m21.py:669\u001b[0m, in \u001b[0;36mMergedMelHarmTokenizer.change_and_describe_tokens_list_at_random_bar\u001b[0;34m(self, harmony_tokens, description_mode)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchange_and_describe_tokens_list_at_random_bar\u001b[39m(\u001b[38;5;28mself\u001b[39m, harmony_tokens, description_mode):\n\u001b[0;32m--> 669\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mharmony_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchange_and_describe_tokens_list_at_random_bar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mharmony_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/maindisk/maximos/repos/TextGuidedMelHarm/harmony_tokenizers_m21.py:1579\u001b[0m, in \u001b[0;36mRootPCTokenizer.change_and_describe_tokens_list_at_random_bar\u001b[0;34m(self, harmony_tokens, description_mode)\u001b[0m\n\u001b[1;32m   1577\u001b[0m         within_bar_end \u001b[38;5;241m=\u001b[39m i\n\u001b[1;32m   1578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(pcs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1579\u001b[0m     chord_token \u001b[38;5;241m=\u001b[39m \u001b[43mget_closes_mir_symbol_for_binpcp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpcs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1580\u001b[0m     \u001b[38;5;66;03m# refresh pcs to reflect new chord\u001b[39;00m\n\u001b[1;32m   1581\u001b[0m     chord_root, bmap, _ \u001b[38;5;241m=\u001b[39m mir_eval\u001b[38;5;241m.\u001b[39mchord\u001b[38;5;241m.\u001b[39mencode( chord_token, reduce_extended_chords\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m )\n",
      "File \u001b[0;32m/media/maindisk/maximos/repos/TextGuidedMelHarm/harmony_tokenizers_m21.py:69\u001b[0m, in \u001b[0;36mget_closes_mir_symbol_for_binpcp\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m     67\u001b[0m k_val_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m all_chords\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m---> 69\u001b[0m     tmp_sum \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogical_and\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_chords\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tmp_sum \u001b[38;5;241m>\u001b[39m k_val_max:\n\u001b[1;32m     71\u001b[0m         k_val_max \u001b[38;5;241m=\u001b[39m tmp_sum\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/numpy/core/fromnumeric.py:2313\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2314\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/numpy/core/fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):  # Number of epochs\n",
    "    train_loss = 0\n",
    "    running_loss = 0\n",
    "    batch_num = 0\n",
    "    running_accuracy = 0\n",
    "    train_accuracy = 0\n",
    "    running_perplexity = 0\n",
    "    train_perplexity = 0\n",
    "    running_token_entropy = 0\n",
    "    train_token_entropy = 0\n",
    "    print('training')\n",
    "    with tqdm(trainloader, unit='batch') as tepoch:\n",
    "        tepoch.set_description(f'Epoch {epoch} | trn')\n",
    "        for batch in tepoch:\n",
    "            model_input_ids = batch['input_ids'].to(device)\n",
    "            melody_attention_mask = batch['attention_mask'].to(device)\n",
    "            harmony_input_ids = batch['harmony_input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            texts = batch['txt']\n",
    "\n",
    "            output = model(\n",
    "                model_input_ids,\n",
    "                melody_attention_mask,\n",
    "                harmony_input_ids,\n",
    "                texts,\n",
    "                labels=labels\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            loss = output['loss']\n",
    "            logits = output['logits']\n",
    "            \n",
    "            loss.backward()  # Compute gradients\n",
    "            optimizer.step()  # Update trainable weights\n",
    "            lr_scheduler.step()  # Update learning rate\n",
    "\n",
    "            # update loss\n",
    "            batch_num += 1\n",
    "            running_loss += loss.item()\n",
    "            train_loss = running_loss/batch_num\n",
    "            # accuracy\n",
    "            predictions = logits.argmax(dim=-1)\n",
    "            mask = labels != -100\n",
    "            running_accuracy += (predictions[mask] == labels[mask]).sum().item()/mask.sum().item()\n",
    "            train_accuracy = running_accuracy/batch_num\n",
    "            # # perplexity\n",
    "            # running_perplexity += perplexity_metric.update(outputs.logits, labels).compute().item()\n",
    "            # train_perplexity = running_perplexity/batch_num\n",
    "            # # token entropy\n",
    "            # _, entropy_per_batch = compute_normalized_token_entropy(outputs.logits, labels, pad_token_id=-100)\n",
    "            # running_token_entropy += entropy_per_batch\n",
    "            # train_token_entropy = running_token_entropy/batch_num\n",
    "            \n",
    "            tepoch.set_postfix(loss=train_loss, accuracy=train_accuracy)\n",
    "    val_loss = 0\n",
    "    running_loss = 0\n",
    "    batch_num = 0\n",
    "    running_accuracy = 0\n",
    "    val_accuracy = 0\n",
    "    running_perplexity = 0\n",
    "    val_perplexity = 0\n",
    "    running_token_entropy = 0\n",
    "    val_token_entropy = 0\n",
    "    print('validation')\n",
    "    with torch.no_grad():\n",
    "        with tqdm(valloader, unit='batch') as tepoch:\n",
    "            tepoch.set_description(f'Epoch {epoch} | val')\n",
    "            for batch in tepoch:\n",
    "                model_input_ids = batch['input_ids'].to(device)\n",
    "                melody_attention_mask = batch['attention_mask'].to(device)\n",
    "                harmony_input_ids = batch['harmony_input_ids'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                texts = batch['txt']\n",
    "\n",
    "                output = model(\n",
    "                    model_input_ids,\n",
    "                    melody_attention_mask,\n",
    "                    harmony_input_ids,\n",
    "                    texts,\n",
    "                    labels=labels\n",
    "                )\n",
    "                loss = output['loss']\n",
    "                logits = output['logits']\n",
    "\n",
    "                # update loss\n",
    "                batch_num += 1\n",
    "                running_loss += loss.item()\n",
    "                val_loss = running_loss/batch_num\n",
    "                # accuracy\n",
    "                predictions = logits.argmax(dim=-1)\n",
    "                mask = labels != -100\n",
    "                running_accuracy += (predictions[mask] == labels[mask]).sum().item()/mask.sum().item()\n",
    "                val_accuracy = running_accuracy/batch_num\n",
    "                # # perplexity\n",
    "                # running_perplexity += perplexity_metric.update(outputs.logits, labels).compute().item()\n",
    "                # val_perplexity = running_perplexity/batch_num\n",
    "                # # token entropy\n",
    "                # _, entropy_per_batch = compute_normalized_token_entropy(outputs.logits, labels, pad_token_id=-100)\n",
    "                # running_token_entropy += entropy_per_batch\n",
    "                # val_token_entropy = running_token_entropy/batch_num\n",
    "                \n",
    "                tepoch.set_postfix(loss=val_loss, accuracy=val_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
